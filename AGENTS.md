<!-- generated by autopsy, updated by Track 23 -->
# NL2SQL Agent

## Overview
Specialized NL2SQL agent for Mako Group's trading desk, converting natural language questions into BigQuery SQL queries. Built on Google ADK with LiteLLM proxy, using a two-layer metadata system (YAML catalog + BQ vector embeddings) for semantic table routing and few-shot retrieval.

## Architecture
```
User Question
    |
    v
root_agent (mako_assistant) -- LlmAgent, delegates trading questions
    |
    v
nl2sql_agent -- LlmAgent with 7 tools, temperature=0.1
    |
    +-> check_semantic_cache --> BQ query_memory (cosine ~0.10 threshold)
    +-> vector_search_tables --> BQ schema_embeddings (VECTOR_SEARCH)
    +-> fetch_few_shot_examples --> BQ query_memory (cached from above)
    +-> load_yaml_metadata --> catalog/ YAML files via catalog_loader.py
    +-> dry_run_sql --> BQ dry run validation
    +-> execute_sql --> BQ query execution (read-only, max 1000 rows)
    +-> save_validated_query --> BQ query_memory INSERT + embedding
```

## Tech Stack
- Language: Python 3.11+
- Framework: Google ADK (Agent Development Kit), LiteLLM proxy
- Database: BigQuery (data warehouse + vector embeddings via ML.GENERATE_EMBEDDING)
- Config: pydantic-settings with .env file
- Logging: structlog (JSON output)
- Testing: pytest, pytest-asyncio (733+ tests)

## Directory Map
| Directory | Purpose |
|-----------|---------|
| nl2sql_agent/ | Main Python package: agent, config, tools, callbacks, prompts |
| nl2sql_agent/tools/ | ADK tool functions: vector search, SQL execution, caching |
| catalog/ | YAML metadata catalog: table/column descriptions, routing rules |
| examples/ | Validated Q->SQL pairs for few-shot retrieval |
| metadata/ | Source-grounded structural indexes, routing guide, field lineage (see below) |
| repos/ | Source repo clones with AGENTS.md documentation cards |
| scripts/ | Embedding pipeline and local dev startup scripts |
| setup/ | Initial BQ dataset/table creation SQL and schema extraction |
| eval/ | Evaluation framework: gold queries, offline/online eval runner |
| tests/ | Unit tests (733+ tests) |
| tests/integration/ | Integration tests requiring live BQ and LiteLLM |
| conductor/ | Development workflow tracking (track-based planning docs) |
| architect/ | Architecture discovery and design documents |
| docs/ | Project documentation and overview materials |

## Metadata Directory (Source-Grounded)

The `metadata/` directory contains structural indexes extracted deterministically from 4 source repos. These are the ground truth for column definitions, transformations, and KPI formulas.

| File | Contents | Source |
|------|----------|--------|
| `proto_fields.yaml` | 45 proto messages, 612 fields, 29 enums, 10 BQ table mappings | `repos/cpp/source/pb/` |
| `data_loader_transforms.yaml` | 10 tables, 739 columns with transformation types (unnest/rename/direct/cast/derive/array_expand) | `repos/data-loader/duckdb_data_loader/` |
| `kpi_computations.yaml` | 5 trade types, 159 formulas, 22 time intervals, 9 fee methods | `repos/kpi/models/` |
| `ROUTING.md` | Cross-repo routing guide: pipeline diagram, per-table routing, concept-to-repo mapping | All 4 repos |
| `field_lineage.yaml` | 30 column lineages tracing proto field -> silver column -> gold formula | Cross-referenced from above 3 YAMLs |

### Data Pipeline (4 repos)
```
CPP repo (proto definitions)
    -> data-library (Go: proto -> Parquet via Kafka)
    -> data-loader (dbt/DuckDB: bronze -> silver transforms)
    -> KPI repo (dbt/BigQuery: silver -> gold computations)
```

### Source Repo Documentation
Each repo under `repos/` has an `AGENTS.md` card documenting purpose, tech stack, directory map, data models, common patterns, and gotchas:
- `repos/cpp/AGENTS.md` — Proto definitions (.proto files for all BQ tables)
- `repos/data-library/AGENTS.md` — Go Kafka consumer, proto-to-Parquet schema handlers
- `repos/data-loader/AGENTS.md` — dbt/DuckDB bronze-to-silver transformation pipeline
- `repos/kpi/AGENTS.md` — dbt/BigQuery KPI computation (instant_edge, PnL, slippage decomposition)

### Key Cross-Repo Facts
- **clicktrade** is NOT a separate proto — it's a filtered subset of `tradedata` (from `PositionEvent` proto)
- **brokertrade** has NO Go schema handler in data-library (`SchemaBuilder: nil`); ingested via `csa_offfloor` Kafka topic
- **contract_size** is streamed from `TradableInstrument.contractSize` proto field, not computed
- **VtCommon** (57 shared fields) is embedded in MarketTrade, QuoterTrade, SwingData, OroSwingData — but NOT in MarketData/MarketDepth
- **oroswingdata** and **swingdata** are separate proto messages with separate pipelines ("oro" in data layer = "oto" in KPI layer)
- **buy_sell_multiplier** signs are REVERSED: markettrade uses -1 for BUY, quotertrade/brokertrade use +1 for BUY

## BQ Tables

### Data Layer (10 tables)
markettrade, quotertrade, brokertrade, swingdata, oroswingdata, marketdata, marketdataext, marketdepth, theodata, tradedata

### KPI Layer (5 tables)
markettrade, quotertrade, brokertrade, clicktrade, otoswing

See `metadata/ROUTING.md` for the full per-table routing (proto message, staging file, KPI calculation file).

## Key Commands
```bash
# Test (unit only)
pytest tests/ -v
# Test (including integration)
pytest -m integration tests/integration/ -v
# Run (web UI)
scripts/start_local.sh
# Run (Docker)
docker compose up
# Eval
python eval/run_eval.py --mode offline
# Embeddings
python scripts/run_embeddings.py --step all
```

## Conventions
- Protocol-based DI: BigQueryProtocol/EmbeddingProtocol for all external services
- `{project}` placeholder in all YAML/SQL, resolved at runtime via settings
- LiteLLM model names MUST include provider prefix (e.g. `openai/gemini-3-flash-preview`)
- All BQ queries use parameterized inputs via query_with_params (SQL injection safe)
- Read-only enforcement: callbacks block DML/DDL at tool entry
- Routing rules live in `catalog/_routing.yaml` and `catalog/{layer}/_dataset.yaml` (single source of truth)
- Pre-commit hooks: ruff lint/format, gitleaks secret scanning, YAML/TOML checks

## Boundaries
- **Always:** use fully-qualified BQ table names, filter on trade_date partition, use protocols not concrete clients
- **Ask first:** changes to routing rules (_routing.yaml), embedding model references, or settings defaults
- **Never:** hardcode GCP project IDs, execute DML/DDL through agent tools, commit .env files with real API keys

## Recent Changes
- 2026-02-21: Track 23 complete — source repo discovery, structural indexes, routing guide, field lineage
- 2026-02-21: Track 17 complete — routing consolidation, pipeline testing
- 2026-02-21: Track 16 complete — repo scaffolding, pre-commit hooks, CI
- 2026-02-20: Track 15 complete — code quality, TypedDict contracts, prompt caching
- 2026-02-20: Track 13 complete — autopsy fixes (critical/high findings)
- 2026-02-20: Autopsy deep review — 68 unique findings across 89 files

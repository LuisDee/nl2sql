# System Architecture

> Generated by `/architect-decompose` on 2026-02-18
> Based on: conductor/product.md, conductor/tech-stack.md, initial-plan.md
> Last synced: 2026-02-19

---

## System Overview

This project implements a specialized Natural Language to SQL (NL2SQL) agent for Mako Group's trading desk. It functions as a **sub-agent** (`nl2sql_agent`) under a root conversational assistant (`mako_assistant`). The root agent handles greetings and delegation, while the NL2SQL agent owns the full data pipeline: routing questions to the correct dataset, retrieving schema metadata, generating BigQuery SQL (via LLM), validating it via dry-runs, and executing it safely.

The system relies on a **Two-Layer Metadata Strategy**:
1.  **Layer 1 (Human):** YAML catalog files defining tables, columns, synonyms, and business rules.
2.  **Layer 2 (AI):** BigQuery Vector Search embeddings of the catalog and validated few-shot examples for semantic retrieval.

---

## Component Map

```
┌─────────────────────────────┐
│  Root Agent (mako_assistant) │
│  Model: LiteLLM proxy        │
└────────────┬────────────────┘
             │ "Show me PnL..." (Delegation)
             ▼
┌──────────────────────────────────────────────────────────────┐
│  NL2SQL Sub-Agent (LlmAgent via LiteLLM)                      │
│                                                                │
│  ┌────────────────────────┐    ┌─────────────────────────────┐│
│  │  Tools Layer (7 tools) │◀──▶│    Context Layer            ││
│  │                        │    │                             ││
│  │ 0. check_semantic_cache│    │ 1. YAML Catalog (catalog/)  ││
│  │ 1. vector_search_tables│    │    - kpi/ (5 tables)        ││
│  │ 2. fetch_few_shot_ex.. │    │    - data/ (7 tables)       ││
│  │ 3. load_yaml_metadata  │    │    - _routing.yaml          ││
│  │ 4. dry_run_sql         │    │                             ││
│  │ 5. execute_sql         │    │ 2. Embeddings (BigQuery)    ││
│  │ 6. save_validated_query│    │    - schema_embeddings      ││
│  │                        │    │    - query_memory           ││
│  └────────┬───────────────┘    └─────────────────────────────┘│
│           │ SQL Generation: LLM (no dedicated tool)           │
│           │ Callbacks: before_tool_guard, after_tool_log      │
└───────────┼──────────────────────────────────────────────────┘
            │ SQL
            ▼
┌──────────────────────────────────────────┐
│            BigQuery                       │
│  KPI dataset: nl2sql_omx_kpi (5 tables)  │
│  Data dataset: nl2sql_omx_data (7 tables) │
│  Metadata: nl2sql_metadata (embeddings)   │
└──────────────────────────────────────────┘
```

---

## Technology Decisions

| Decision | Choice | Rationale |
|----------|--------|-----------|
| **Agent Framework** | Google ADK | Native `LlmAgent` support for sub-agent delegation and tool use. |
| **LLM Routing** | LiteLLM proxy | OpenAI-compatible API; decouples agent from specific model provider. Model names need `openai/` prefix. |
| **Configuration** | pydantic-settings | Type-safe settings from `.env`, with validation. Singleton `Settings()` instance. |
| **Vector Database** | BigQuery Vector Search | Keeps data and embeddings in the same place; `VECTOR_SEARCH` directly in SQL. |
| **Metadata Source** | YAML + Git | Version-controlled source of truth, easier for humans to edit. |
| **SQL Validation** | BigQuery Dry-Run | Zero-cost syntax/permission check before execution. |
| **SQL Generation** | LLM-native (no tool) | The LLM generates SQL directly from metadata + few-shot context. No dedicated `generate_sql` tool — simpler and more flexible. |
| **Dependency Injection** | Protocol-based + module-level DI | `BigQueryProtocol` for abstractions. `_deps.py` for shared singleton service. |

---

## Architecture Decision Records

### ADR-001: Separation of Root and Sub-Agent

- **Status:** Accepted, Implemented
- **Context:** Traders need a single interface but the data pipeline is complex.
- **Decision:** Use a **Root Agent** for intent classification and a dedicated **NL2SQL Sub-Agent** for data tasks.
- **Consequences:**
    - (+) Clean separation of concerns (chat vs. query).
    - (+) Sub-agent prompt can be highly specialized for SQL without distraction.
    - (+) Root agent can handle other future domains (Compliance, Risk) by adding more sub-agents.

### ADR-002: BigQuery-Native Embeddings

- **Status:** Accepted, Implemented
- **Context:** We need semantic search for tables and few-shot examples.
- **Decision:** Use **BigQuery Vector Search** (`VECTOR_SEARCH` function) instead of a separate vector DB.
- **Consequences:**
    - (+) Simplifies infra (no new service).
    - (+) Atomic updates (metadata and embeddings live together).
    - (+) Agent tools just run SQL queries to find context.

### ADR-003: Tools-First Architecture (not Service-Class Layer)

- **Status:** Accepted, Implemented
- **Context:** The initial plan described a service-class layer (SQLGenerator, MetadataCatalog, SQLExecutor, SessionManager). During implementation, plain tool functions proved simpler and aligned better with ADK's function-based tool model.
- **Decision:** Use **standalone Python functions** as ADK tools with shared DI via `_deps.py`, rather than service classes.
- **Consequences:**
    - (+) Simpler — each tool is a single function with clear inputs/outputs.
    - (+) Direct alignment with ADK's `FunctionTool` / plain-function approach.
    - (+) DI via `_deps.py` avoids constructor complexity while preserving testability.
    - (-) No formal service layer for reuse outside ADK context (acceptable for current scope).

### ADR-004: LLM-Native SQL Generation

- **Status:** Accepted, Implemented
- **Context:** The initial plan included a `generate_sql` tool that would call the LLM explicitly. In practice, ADK's `LlmAgent` already handles generation — the agent writes SQL as part of its natural conversation flow.
- **Decision:** No dedicated `generate_sql` tool. The agent's system prompt plus metadata/examples context enables direct SQL generation.
- **Consequences:**
    - (+) One fewer tool call per question (lower latency).
    - (+) The LLM sees the full conversation context when generating SQL.
    - (-) Less explicit control over the generation step (mitigated by dry-run validation).

### ADR-005: Semantic Cache as First Tool

- **Status:** Accepted, Implemented
- **Context:** Traders often repeat similar questions. The initial plan listed semantic caching as a deferred Phase F feature.
- **Decision:** Implemented `check_semantic_cache` as the first tool in the pipeline (step 0). Uses VECTOR_SEARCH against query_memory with a tight cosine distance threshold.
- **Consequences:**
    - (+) Cache hits skip the entire vector search + metadata + generation pipeline.
    - (+) Significant latency reduction for repeated/similar questions.

---

## Accepted Architecture Patterns

| Pattern | Tier | Status |
|---------|------|--------|
| **Retry with Circuit Breaker** | Implemented | `after_tool_log` tracks dry-run failures; hard stop at 3 retries via `before_tool_guard`. |
| **Structured Logging** | Implemented | `structlog` JSON output with trace context in `logging_config.py`. |
| **Input Validation** | Implemented | Pydantic `Settings` for config; tool-level validation in callbacks + each tool function. |
| **DML Guard** | Implemented | Both `before_tool_guard` callback and `execute_sql` reject non-SELECT queries. |
| **Session State** | Implemented | `after_tool_log` persists last query SQL + results summary for follow-up questions. |
| **Combined Vector Search** | Implemented | Single BQ round-trip generates embedding once, searches both schema_embeddings and query_memory. |
| **Global Tool Call Limit** | Implemented | `before_tool_guard` enforces `max_tool_calls_per_turn` (default 15) to prevent infinite loops. |

---

## Deferred Pattern Triggers

| Pattern | Trigger Condition | Discovery Classification | Status |
|---------|-------------------|--------------------------|--------|
| **Semantic Caching** | Latency > 5s for repeated queries | NEW_TRACK (Optimization) | IMPLEMENTED (Track 05) |
| **LoopAgent (Auto-Correction)** | Dry-run failure rate > 10% | ARCHITECTURE_CHANGE | Replaced by callback-based retry + circuit breaker |
| **Loop Fix & Performance** | Infinite reasoning loops detected | NEW_TRACK | Track 08 (pending) |

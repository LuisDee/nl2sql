<!-- generated by autopsy -->
# NL2SQL Agent

## Overview
Specialized NL2SQL agent for Mako Group's trading desk, converting natural language questions into BigQuery SQL queries. Built on Google ADK with LiteLLM proxy, using a two-layer metadata system (YAML catalog + BQ vector embeddings) for semantic table routing and few-shot retrieval.

## Architecture
```
User Question
    |
    v
root_agent (mako_assistant) -- LlmAgent, delegates trading questions
    |
    v
nl2sql_agent -- LlmAgent with 7 tools, temperature=0.1
    |
    +-> check_semantic_cache --> BQ query_memory (cosine ~0.10 threshold)
    +-> vector_search_tables --> BQ schema_embeddings (VECTOR_SEARCH)
    +-> fetch_few_shot_examples --> BQ query_memory (cached from above)
    +-> load_yaml_metadata --> catalog/ YAML files via catalog_loader.py
    +-> dry_run_sql --> BQ dry run validation
    +-> execute_sql --> BQ query execution (read-only, max 1000 rows)
    +-> save_validated_query --> BQ query_memory INSERT + embedding
```

## Tech Stack
- Language: Python 3.11+
- Framework: Google ADK (Agent Development Kit), LiteLLM proxy
- Database: BigQuery (data warehouse + vector embeddings via ML.GENERATE_EMBEDDING)
- Config: pydantic-settings with .env file
- Logging: structlog (JSON output)
- Testing: pytest, pytest-asyncio

## Directory Map
| Directory | Purpose |
|-----------|---------|
| nl2sql_agent/ | Main Python package: agent, config, tools, callbacks, prompts |
| nl2sql_agent/tools/ | ADK tool functions: vector search, SQL execution, caching |
| catalog/ | YAML metadata catalog: table/column descriptions, routing rules |
| examples/ | Validated Q->SQL pairs for few-shot retrieval |
| scripts/ | Embedding pipeline and local dev startup scripts |
| setup/ | Initial BQ dataset/table creation SQL and schema extraction |
| eval/ | Evaluation framework: gold queries, offline/online eval runner |
| tests/ | Unit tests (155+ tests) |
| tests/integration/ | Integration tests requiring live BQ and LiteLLM |
| conductor/ | Development workflow tracking (track-based planning docs) |
| architect/ | Architecture discovery and design documents |
| docs/ | Project documentation and overview materials |

## Key Commands
```bash
# Test (unit only)
pytest tests/ -v
# Test (including integration)
pytest -m integration tests/integration/ -v
# Run (web UI)
scripts/start_local.sh
# Run (Docker)
docker compose up
# Eval
python eval/run_eval.py --mode offline
# Embeddings
python scripts/run_embeddings.py --step all
```

## Conventions
- Protocol-based DI: BigQueryProtocol/EmbeddingProtocol for all external services
- `{project}` placeholder in all YAML/SQL, resolved at runtime via settings
- LiteLLM model names MUST include provider prefix (e.g. `openai/gemini-3-flash-preview`)
- All BQ queries use parameterized inputs via query_with_params (SQL injection safe)
- Read-only enforcement: callbacks block DML/DDL at tool entry

## Boundaries
- **Always:** use fully-qualified BQ table names, filter on trade_date partition, use protocols not concrete clients
- **Ask first:** changes to routing rules (_routing.yaml), embedding model references, or settings defaults
- **Never:** hardcode GCP project IDs, execute DML/DDL through agent tools, commit .env files with real API keys

## Known Issues

**Critical (10 findings):**
- Production LiteLLM API key committed in git history (`conductor/archive/01_foundation/plan.md`, `.env.prod`) -- rotate immediately
- `ARRAY_LENGTH(NULL)` bug in `learning_loop.py:31` and `run_embeddings.py:243,256,269` -- embedding generation is a no-op for all newly inserted rows
- Circuit breaker state (`max_retries_reached`) not reset on new question in `callbacks.py:82-84` -- agent permanently broken after 3 dry-run failures
- Example SQL uses wrong column names (`edge` instead of `instant_edge`, `bid_size_0` instead of `bid_volume_0`, `putcall` instead of `option_type_name`) across `kpi_examples.yaml` and `data_examples.yaml`
- "Total PnL" example and routing rule double-counts by UNION ALL-ing markettrade with Mako-specific tables
- Online eval calls `nl2sql_agent.run()` which does not exist on ADK LlmAgent (`eval/run_eval.py:342`)
- Module-level BQ client initialization in `agent.py:35-38` crashes import without GCP credentials
- `execute_query()` in `clients.py:42-47` has no timeout -- agent can hang indefinitely
- KPI YAML files (200-376KB each) send all 800+ columns to LLM per metadata load
- `data/_dataset.yaml` lists `brokertrade` but no `catalog/data/brokertrade.yaml` exists

**High (20 findings):** SQL read-only guard bypassable via `WITH...INSERT`, naive LIMIT detection, combined vector search drops metadata columns, routing MERGE uses ON FALSE, CREATE OR REPLACE TABLE destroys data, embedding pipeline has no error handling, string interpolation without escaping in populate_embeddings.py, Dockerfile broken (editable install before source exists), Docker runs as root with no .dockerignore

See `.autopsy/REVIEW_REPORT.md` for the complete list of 68 deduplicated findings.

## Technical Debt

1. **Routing intelligence fragmentation:** Rules duplicated across `_routing.yaml`, `kpi/_dataset.yaml`, `data/_dataset.yaml`, and `prompts.py` with no drift detection
2. **No dependency lock file:** All deps use unbounded `>=` pins; `numpy` used but not declared
3. **Module-level side effects:** `agent.py` import triggers BQ client init; `Settings()` runs at import time
4. **Misplaced integration tests:** 2 files in `tests/integration/` use only mocks, excluded from default runs
5. **KPI YAML duplication:** ~80% of each KPI table YAML is duplicated shared columns across 5 files (~20,000 redundant lines)
6. **No pre-commit secret scanning:** No `detect-secrets` or `gitleaks` hook configured
7. **Embedding pipeline untested:** `run_embeddings.py` and `populate_embeddings.py` have zero unit tests

## Recent Changes
- 2026-02-20: Autopsy deep review completed -- 68 unique findings across 89 files
- 2026-02-20: Initial documentation generated by autopsy

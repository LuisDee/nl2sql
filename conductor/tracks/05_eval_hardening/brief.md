# Track Brief: Eval & Hardening (Phase E/F)

> Generated by Architect based on `initial-plan.md`

## What This Track Delivers
This track implements the evaluation and production-readiness layer. It builds a **Gold Standard Evaluation Set** of 50 questions with expected SQL, implements an automated `run_eval.py` runner to measure accuracy, and adds the **LoopAgent** pattern for automatic retries on dry-run failures. It also implements **Semantic Caching** for repeated questions and ensures the **Learning Loop** is active in the UX.

## Source Requirements (from initial-plan.md)
- **E.1 Gold Eval Set:** `eval/gold_queries.yaml` with 50 diverse questions.
- **E.2 Eval Runner:** `eval/run_eval.py` measuring SQL syntax validity, routing accuracy, and result correctness.
- **E.3 Retry Logic:** Wrap `sql_generator` in a `LoopAgent` to auto-correct dry-run errors (up to 3 attempts).
- **F.1 Learning Loop UX:** Prompt user "Was this correct?" -> call `save_validated_query`.
- **F.2 Semantic Cache:** Check `query_memory` for highly similar questions before generating.
- **F.3 Session State:** Store `last_query` for follow-ups.

## Cross-Cutting Constraints
- **Performance:** Caching should return < 2s.
- **Accuracy:** Target > 90% routing accuracy on gold set.

## Interface Contracts
- **Owned:** `run_eval.py`, `LoopAgent`
- **Consumed:** `All Tools`, `BigQuery`

## Key Design Decisions
1.  **Eval Metric:** String match or result match? -> **Result Match** (run both queries and compare results) is more robust.
2.  **Retry Strategy:** Simple loop or LLM reasoning? -> **LoopAgent** (LLM sees error, re-generates).

## Test Strategy
- **Integration:** Run `run_eval.py` against `dev_agent_test`.
- **Manual:** Verify retry works by asking a deliberately tricky question that might cause a syntax error first.

## Complexity: Large
## Estimated Phases: 4

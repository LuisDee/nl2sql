# Track Brief: Embedding Strategy & Glossary Collection

> Generated by Architect based on Best Practice Alignment Plan (Part 2 + Part 3)

## What This Track Delivers
This track redesigns the embedding strategy to separate **retrieval signals** from **generation context**, addressing the embedding dilution problem identified in research. It also creates a **business glossary collection** in BQ for domain-specific concepts that don't map directly to columns (e.g., "total PnL", "edge", "slippage decomposition", "ATM strike").

### Key Changes
1. **Embedding text rewrite:** Current `populate_embeddings.py` only includes `name, type, layer, description, synonyms` in embedding text. This track adds `category` and `example_values` (from Track 18) to the retrieval signal, while keeping verbose `formula` and `related_columns` out of the embedding (they go in the generation context only).
2. **Glossary embeddings:** New BQ table `{project}.metadata.glossary_embeddings` with concept→definition→related_columns mappings. Enables the agent to resolve business concepts like "total PnL" to the correct column combination.
3. **Enriched vector search CTE:** Update `vector_search_columns` tool to optionally search the glossary collection alongside schema embeddings, returning both column matches and concept matches.

## Source Requirements
- **Part 2 — Embedding Strategy:**
  - Separate retrieval text (name + type + category + example_values + synonyms) from generation context (formula + related_columns + description)
  - Avoid embedding dilution: long descriptions hurt retrieval precision
  - Rebuild column embeddings with enriched retrieval text after Track 18 fields are populated
- **Part 3 — Business Glossary Collection:**
  - Define 20-30 key business concepts with definitions and related columns
  - Store in BQ with embeddings for vector search
  - Concepts: PnL variants, edge metrics, slippage decomposition, trade type taxonomy, timestamp conventions, ATM strike logic

## Cross-Cutting Constraints
- **Depends on Track 18:** Needs `category` and `example_values` fields populated in YAML before rebuilding embeddings
- **Embedding model:** Uses same Vertex AI model (`text-embedding-005`) as existing pipeline
- **Backwards compatible:** `vector_search_columns` tool must still work with old embeddings during migration
- **No prompt changes:** Glossary results are injected via tool output, not hardcoded in system prompt

## Interface Contracts
- **Owned:**
  - `enriched_embeddings` — rebuilt column embeddings with category + example_values in retrieval text
  - `glossary_embeddings` — new BQ table with business concept embeddings
  - `enriched_vector_search_cte` — updated CTE that searches both collections
- **Consumed:**
  - `enriched_yaml_schema` — Track 18's enriched YAML with category, example_values
  - `column_embeddings` — existing embedding pipeline (`scripts/run_embeddings.py`)
  - `vector_search_columns` — existing tool in `nl2sql_agent/tools/`
  - `bigquery` — BQ access for glossary table creation and embedding generation

## Key Design Decisions
1. **Retrieval vs generation split:** Embedding text = `{name} | {type} | {category} | {synonyms} | {example_values}`. Generation context (returned in tool output) = full description + formula + related_columns. This keeps embeddings focused on searchable signals.
2. **Glossary as separate collection:** Rather than cramming business concepts into column embeddings, maintain a separate `glossary_embeddings` table. The vector search tool does a UNION of both result sets, deduped by relevance score.
3. **Incremental rebuild:** Provide a `--rebuild` flag in `run_embeddings.py` that regenerates all embeddings using the new text template. Old embeddings remain until explicitly rebuilt.

## Test Strategy
- **Unit:** New embedding text builder function tested with sample enriched YAML
- **Unit:** Glossary YAML/JSON validated for required fields (concept, definition, related_columns)
- **Integration:** Vector search returns glossary matches for "total PnL" query
- **Integration:** Rebuilt embeddings improve retrieval precision on eval set (measure before/after)

## Complexity: Large
## Estimated Phases: 4

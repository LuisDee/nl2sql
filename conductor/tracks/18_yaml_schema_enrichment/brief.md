# Track Brief: YAML Schema Enrichment & Data Profiling

> Generated by Architect based on Best Practice Alignment Plan (Part 1 + Part 5)

## What This Track Delivers
This track enriches the existing YAML catalog with structured metadata fields inspired by Snowflake Cortex Analyst's semantic view specification. It adds `category`, `formula`, `example_values`, `related_columns`, `typical_aggregation`, and `filterable` fields to column definitions across all 12 table YAMLs. It also builds a BQ data profiling pipeline that auto-populates `example_values` for categorical/enum columns by querying `APPROX_TOP_COUNT` on live data.

## Source Requirements
- **Part 1 — YAML Metadata Schema Enrichment:**
  - Add `category` field to every column (one of: dimension, measure, time, identifier)
  - Add `formula` field to computed columns explaining derivation logic
  - Add `example_values` for categorical/enum columns (from BQ profiling)
  - Add `related_columns` for columns that are commonly used together
  - Add `typical_aggregation` for measures (SUM, AVG, WEIGHTED_AVG, etc.)
  - Add `filterable: true/false` to indicate whether columns are useful as WHERE predicates
- **Part 5 — Auto-Generation Pipeline:**
  - `scripts/profile_columns.py` — queries BQ `APPROX_TOP_COUNT` for low-cardinality columns
  - Pydantic model for enriched column schema (validates all new fields)
  - LLM-assisted enrichment for `formula` and `related_columns` (optional, can be manual)

## Cross-Cutting Constraints
- **Backwards compatibility:** All new fields are optional — existing YAML parsing must not break
- **Validation:** Extend `test_yaml_catalog.py` to validate new fields when present
- **Schema ground truth:** `schemas/{layer}/*.json` files (committed in metadata validation track) are the source of truth for column names
- **No YAML bloat:** KPI tables already have 200-376KB YAMLs with 800+ columns each. New fields must be concise (no multi-paragraph descriptions)

## Interface Contracts
- **Owned:**
  - `enriched_yaml_schema` — YAML catalog files with new fields (category, formula, example_values, related_columns, typical_aggregation, filterable)
  - `data_profiling_pipeline` — `scripts/profile_columns.py` that queries BQ for categorical value distributions
- **Consumed:**
  - `yaml_catalog` — existing catalog files in `catalog/{kpi,data}/*.yaml`
  - `bigquery` — live BQ access for data profiling
  - `column_embeddings` — existing column embedding text template (will be consumed by Track 19)

## Key Design Decisions
1. **Category taxonomy:** Use Cortex Analyst categories (`dimension`, `measure`, `time`, `identifier`) rather than inventing custom ones. Maps cleanly to aggregation and filter behaviour.
2. **Profiling threshold:** Columns with <50 distinct values get auto-profiled for `example_values`. Above that, they're continuous/high-cardinality and don't benefit from enumeration.
3. **Pydantic validation:** Add a `ColumnSchema` Pydantic model that validates enriched YAML on load, catching malformed entries early.
4. **Phased rollout:** Phase 1 adds the schema + profiling script. Phase 2 enriches KPI tables (5). Phase 3 enriches data tables (7). Phase 4 adds validation tests.

## Test Strategy
- **Unit:** Pydantic model validates sample enriched YAML correctly
- **Unit:** `test_yaml_catalog.py` extended to validate `category` is one of allowed values, `typical_aggregation` is valid, etc.
- **Integration:** `profile_columns.py` runs against dev BQ and produces expected output for known categorical columns (e.g., `portfolio`, `symbol`)
- **Regression:** All 578+ existing tests continue to pass (new fields are optional)

## Complexity: Large
## Estimated Phases: 4

<!-- generated by autopsy -->
# Agent Tools

## Purpose
ADK tool functions that the nl2sql_agent calls in sequence: semantic cache check, vector search for table routing, YAML metadata loading, SQL validation (dry run), SQL execution, and learning loop (save validated queries). All tools share a BigQuery service via module-level dependency injection in _deps.py.

## Key Files
| File | Purpose | Key Exports |
|------|---------|-------------|
| __init__.py | Re-exports all tools and init_bq_service | All tool functions |
| _deps.py | Module-level BQ service holder + per-question vector cache | `init_bq_service`, `get_bq_service`, `cache_vector_result` |
| vector_search.py | Combined VECTOR_SEARCH for tables + examples in one BQ round-trip | `vector_search_tables`, `fetch_few_shot_examples` |
| metadata_loader.py | Loads YAML catalog by table name, resolves dataset ambiguity | `load_yaml_metadata` |
| semantic_cache.py | Checks query_memory for near-exact match (cosine < 0.10) | `check_semantic_cache` |
| sql_validator.py | BQ dry run validation, returns estimated bytes | `dry_run_sql` |
| sql_executor.py | Executes SELECT queries with read-only guard and row limit | `execute_sql` |
| learning_loop.py | Inserts validated Q->SQL to query_memory + generates embedding | `save_validated_query` |

## Data Flow
question -> check_semantic_cache -> vector_search_tables (+ caches examples) -> load_yaml_metadata -> dry_run_sql -> execute_sql -> save_validated_query

## Dependencies
- Internal: config.py (settings), catalog_loader.py (YAML loading), serialization.py (row sanitization)
- External: BigQuery (via BigQueryProtocol through _deps.py)

## Patterns & Conventions
- All tools are plain functions; ADK wraps them as FunctionTool automatically
- vector_search.py uses a combined CTE to generate one embedding, search two tables
- _deps.py caches vector results so fetch_few_shot_examples is a Python cache hit
- SQL templates use .format() for metadata refs but @params for user input (injection-safe)

## Configuration
- `settings.vector_search_top_k` (default 5), `settings.semantic_cache_threshold` (default 0.10)
- `settings.bq_max_result_rows` (default 1000), `settings.bq_query_timeout_seconds` (default 30)

## Gotchas
- _deps.py uses module-level globals for BQ service and vector cache -- not thread-safe
- vector_search.py .format() for metadata_dataset/embedding_model is safe (from settings, not user input)
- learning_loop.py executes INSERT + UPDATE (DML) -- the ONLY tool that writes to BQ
- execute_sql auto-appends LIMIT if not present (simple string check, not AST-aware)

## Testing
- Tests: tests/test_vector_search.py, test_sql_executor.py, test_sql_validator.py, test_semantic_cache.py, test_learning_loop.py, test_combined_vector_search.py
- Run: `pytest tests/ -v`
- Gaps: No test for vector_search fallback path (combined query failure -> schema-only)

## Boundaries
- **Always:** use get_bq_service() not direct BQ client, return dict with 'status' key
- **Ask first:** changes to SQL templates or vector search top_k
- **Never:** add DML tools beyond save_validated_query, bypass read-only guard

## Recent Changes
- 2026-02-20: Initial documentation generated by autopsy

<!-- generated by autopsy -->
# Evaluation Framework

## Purpose
Evaluation framework for measuring NL2SQL agent accuracy. Contains a gold standard query set (50+ queries across 9 categories and 13 tables), offline evaluation (structure + dry-run validation), and online evaluation (full agent pipeline with result comparison). Also includes gold set validation tooling.

## Key Files
| File | Purpose | Key Exports |
|------|---------|-------------|
| run_eval.py | Offline/online eval runner, metrics, report generation | `EvalRunner`, `EvalReport`, `EvalResult`, `compute_component_match` |
| gold_queries.yaml | 50+ gold standard Q->SQL pairs with categories and expected tables | YAML data |
| validate_gold_set.py | Validates gold set completeness: all tables, categories, required fields | `validate_gold_set` |
| adk/routing_eval.test.json | ADK-format eval test cases for routing accuracy | JSON test data |

## Data Flow
gold_queries.yaml -> EvalRunner.run_offline() -> resolve {project} -> extract tables -> compare vs expected -> EvalReport.to_markdown()

## Dependencies
- Internal: nl2sql_agent.catalog_loader (resolve_example_sql), nl2sql_agent.config (settings)
- External: pandas (result comparison), pyyaml (gold query loading)

## Patterns & Conventions
- Gold SQL uses {project} placeholder resolved at runtime (same as catalog)
- Component match compares: tables, SELECT columns, GROUP BY/ORDER BY/WHERE presence
- Online mode patches execute_sql to capture generated SQL before execution
- EvalReport has computed properties: routing_accuracy, syntax_accuracy, execution_accuracy

## Gotchas
- Online mode monkey-patches nl2sql_agent.tools list (line 328) -- fragile if tool order changes
- compare_result_sets normalizes column order and rounds floats but is sensitive to type mismatches
- validate_gold_set expects ALL_TABLES (9 unique names) but some exist in both datasets
- Exit code depends on accuracy thresholds: offline requires 100% routing, online requires >90%

## Testing
- Tests: tests/test_eval_runner.py (unit), tests/integration/test_eval_gold_set.py
- Run: `python eval/run_eval.py --mode offline`

## Boundaries
- **Always:** use {project} placeholder in gold SQL, include expected_tables for every query
- **Ask first:** changes to accuracy thresholds or component match scoring
- **Never:** remove gold queries without replacement, hardcode project IDs in gold SQL

## Recent Changes
- 2026-02-20: Initial documentation generated by autopsy
